# SPDX-FileCopyrightText: 2026 Greg Brandt <brandt.greg@gmail.com>
#
# SPDX-License-Identifier: Apache-2.0

"""
High-priority tests based on coverage analysis from TEST_PLAN.md.
"""

import pytest
import tempfile
import os
from avrokit.tools.count import CountTool
from avrokit.tools.stats import StatsTool, Stats
from avrokit.tools.concat import ConcatTool
from avrokit.tools.cat import CatTool
from avrokit.io import avro_schema, avro_writer, avro_reader
from avrokit.url.factory import parse_url
from avrokit.io.writer import PartitionedAvroWriter
from avrokit.asyncio.reader import BlockingQueueAvroReader
from avrokit.asyncio.writer import DeferredAvroWriter


def create_test_avro_file(tmpdir, filename, schema, records, codec="null"):
    """Helper to create test Avro files."""
    file_path = os.path.join(tmpdir, filename)
    url = parse_url(file_path)
    with avro_writer(url.with_mode("wb"), schema, codec=codec) as writer:
        for record in records:
            writer.append(record)
    return file_path


class TestCountTool:
    """Tests for the count tool (Priority 1 - 0% coverage)."""

    def test_count_single_file(self):
        """Basic counting of records in a single file."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(
                tmpdir, "test.avro", schema, [{"id": i} for i in range(10)]
            )
            url = parse_url(file_path)

            tool = CountTool()
            count = tool.count([url])

            assert count == 10

    def test_count_multiple_files(self):
        """Glob pattern support - counting across multiple files."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            # Create multiple files
            for i in range(3):
                create_test_avro_file(
                    tmpdir,
                    f"test_{i}.avro",
                    schema,
                    [{"id": j + i * 10} for j in range(10)],
                )

            # Count using glob pattern
            pattern = os.path.join(tmpdir, "test_*.avro")
            url = parse_url(pattern)

            tool = CountTool()
            count = tool.count(url.expand())

            assert count == 30

    def test_count_empty_file(self):
        """Empty file returns 0."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(tmpdir, "empty.avro", schema, [])
            url = parse_url(file_path)

            tool = CountTool()
            count = tool.count([url])

            assert count == 0

    def test_count_single_record(self):
        """Edge case: single record file."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "value", "type": "string"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(
                tmpdir, "single.avro", schema, [{"value": "hello"}]
            )
            url = parse_url(file_path)

            tool = CountTool()
            count = tool.count([url])

            assert count == 1

    def test_count_deflate_codec(self):
        """Different codec handling (deflate)."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(
                tmpdir,
                "deflate.avro",
                schema,
                [{"id": i} for i in range(5)],
                codec="deflate",
            )
            url = parse_url(file_path)

            tool = CountTool()
            count = tool.count([url])

            assert count == 5

    def test_fast_count_records(self):
        """Test the core method directly."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(
                tmpdir, "test.avro", schema, [{"id": i} for i in range(25)]
            )
            url = parse_url(file_path)

            with avro_reader(url.with_mode("rb")) as reader:
                tool = CountTool()
                count = tool.fast_count_records(reader)

            assert count == 25


class TestStatsTool:
    """Tests for the stats tool (Priority 1 - 47% coverage)."""

    def test_stats_null_field_counts(self):
        """Null count per field."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [
                    {"name": "id", "type": "int"},
                    {"name": "name", "type": ["string", "null"]},
                ],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [
                {"id": 1, "name": "Alice"},
                {"id": 2, "name": None},
                {"id": 3, "name": "Bob"},
            ]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)
            url = parse_url(file_path)

            tool = StatsTool()
            tool.run.__self__.run

            # Replicate the tool's run logic
            stats_obj = Stats()
            with avro_reader(url) as reader:
                for record in reader:
                    stats_obj.count += 1
                    for f, v in record.items():
                        if f not in stats_obj.count_null_by_field:
                            stats_obj.count_null_by_field[f] = 0
                        if v is None:
                            stats_obj.count_null_by_field[f] += 1

            assert stats_obj.count == 3
            assert stats_obj.count_null_by_field.get("name") == 1

    def test_stats_multiple_files(self):
        """Aggregation across multiple files."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            urls = []
            for i in range(3):
                file_path = create_test_avro_file(
                    tmpdir, f"file_{i}.avro", schema, [{"id": j} for j in range(5)]
                )
                urls.append(parse_url(file_path))

            total_count = 0
            for url in urls:
                with avro_reader(url) as reader:
                    for _ in reader:
                        total_count += 1

            assert total_count == 15

    def test_stats_nested_types(self):
        """Complex schemas with nested records and arrays."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [
                    {"name": "id", "type": "int"},
                    {"name": "items", "type": {"type": "array", "items": "int"}},
                    {
                        "name": "nested",
                        "type": [
                            {
                                "type": "record",
                                "name": "Nested",
                                "fields": [{"name": "value", "type": "string"}],
                            },
                            "null",
                        ],
                    },
                ],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [
                {"id": 1, "items": [1, 2, 3], "nested": {"value": "a"}},
                {"id": 2, "items": [], "nested": None},
            ]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)
            url = parse_url(file_path)

            with avro_reader(url) as reader:
                count = sum(1 for _ in reader)

            assert count == 2

    def test_stats_json_output_structure(self):
        """Verify output format."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(
                tmpdir, "test.avro", schema, [{"id": i} for i in range(3)]
            )
            url = parse_url(file_path)

            stats = Stats()
            with avro_reader(url) as reader:
                for record in reader:
                    stats.count += 1
                    url_size = url.size()
                    stats.size_bytes += url_size

            result = {
                "count": stats.count,
                "count_by_file": stats.count_by_file,
                "count_null_by_field": stats.count_null_by_field,
                "size_bytes": stats.size_bytes,
                "size_bytes_by_file": stats.size_bytes_by_file,
            }

            assert "count" in result
            assert "size_bytes" in result
            assert result["count"] == 3
            assert result["size_bytes"] > 0

    def test_stats_empty_file(self):
        """Zero records but file size present."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(tmpdir, "empty.avro", schema, [])
            url = parse_url(file_path)

            stats = Stats()
            with avro_reader(url) as reader:
                for _ in reader:
                    stats.count += 1

            url_size = url.size()

            assert stats.count == 0
            assert url_size > 0


class TestGetMetaTool:
    """Tests for getmeta tool (Priority 2 - 47% coverage)."""

    def test_getmeta_standard_metadata(self):
        """Standard metadata: avro.schema, avro.codec."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, [{"id": 1}])
            url = parse_url(file_path)

            with avro_reader(url.with_mode("rb")) as reader:
                meta = reader.meta

            assert "avro.schema" in meta
            assert "avro.codec" in meta

    def test_getmeta_custom_metadata(self):
        """User-defined custom metadata."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = os.path.join(tmpdir, "test.avro")
            url = parse_url(file_path)

            # Write custom metadata using DataFileWriter directly
            from avro.datafile import DataFileWriter
            from avro.io import DatumWriter

            with url.with_mode("wb").open() as f:
                writer = DataFileWriter(f, DatumWriter(), schema, codec="null")
                writer.append({"id": 1})
                # Add custom metadata before closing
                writer.meta["custom.key"] = b"custom_value"
                writer.close()

            with avro_reader(url.with_mode("rb")) as reader:
                meta = reader.meta

            assert "custom.key" in meta
            assert meta["custom.key"] == b"custom_value"

    def test_getmeta_utf8_special_chars(self):
        """Encoding edge cases with special UTF-8 characters."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "text", "type": "string"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [
                {"text": "hello"},
                {"text": "„Åì„Çì„Å´„Å°„ÅØ"},
                {"text": "üéâ"},
            ]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)
            url = parse_url(file_path)

            with avro_reader(url.with_mode("rb")) as reader:
                schema_bytes = reader.meta.get("avro.schema")
                assert schema_bytes is not None
                schema_str = schema_bytes.decode("utf-8")
                assert "Test" in schema_str


class TestConcatTool:
    """Tests for concat tool (Priority 2 - 19% coverage)."""

    def test_concat_schema_mismatch(self):
        """Falls back to record concat when schemas differ."""
        schema1 = avro_schema(
            {
                "type": "record",
                "name": "Test1",
                "fields": [{"name": "id", "type": "int"}],
            }
        )
        schema2 = avro_schema(
            {
                "type": "record",
                "name": "Test2",
                "fields": [{"name": "value", "type": "string"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            # Create files with different schemas
            file1 = create_test_avro_file(tmpdir, "file1.avro", schema1, [{"id": 1}])
            file2 = create_test_avro_file(
                tmpdir, "file2.avro", schema2, [{"value": "hello"}]
            )

            tool = ConcatTool()
            urls = [parse_url(file1), parse_url(file2)]

            # check_schema_and_codec returns False for different schemas
            # This causes the fallback to record concat instead of block concat
            result = tool.check_schema_and_codec(urls, "null")
            assert not result

            # Verify the tool correctly identifies schema mismatch
            # by checking that it falls back to record concat (not block concat)
            # The run method checks schema and codec match before deciding

    def test_concat_codec_mismatch(self):
        """Falls back correctly when codecs differ."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            # Create files with different codecs
            file1 = create_test_avro_file(
                tmpdir, "file1.avro", schema, [{"id": 1}], codec="null"
            )
            file2 = create_test_avro_file(
                tmpdir, "file2.avro", schema, [{"id": 2}], codec="deflate"
            )

            output_file = os.path.join(tmpdir, "output.avro")

            tool = ConcatTool()
            urls = [parse_url(file1), parse_url(file2)]
            output_url = parse_url(output_file)

            # check_schema_and_codec should return False
            result = tool.check_schema_and_codec(urls, "null")
            assert not result

            # Should fall back to record concat
            tool.concat(urls, output_url, "null")

            with avro_reader(output_url.with_mode("rb")) as reader:
                records = list(reader)
                assert len(records) == 2

    def test_concat_block_concat_success(self):
        """Fast path works when schemas and codecs match."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            # Create multiple files with same schema and codec
            files = []
            for i in range(3):
                file_path = create_test_avro_file(
                    tmpdir,
                    f"file{i}.avro",
                    schema,
                    [{"id": j} for j in range(5)],
                    codec="null",
                )
                files.append(parse_url(file_path))

            output_file = os.path.join(tmpdir, "output.avro")
            output_url = parse_url(output_file)

            tool = ConcatTool()

            # check_schema_and_codec should return True
            result = tool.check_schema_and_codec(files, "null")
            assert result

            # Use block concat (fast path)
            tool.block_concat(files, output_url, "null")

            with avro_reader(output_url.with_mode("rb")) as reader:
                records = list(reader)
                assert len(records) == 15

    def test_concat_single_file(self):
        """Edge case: single file concatenation."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file1 = create_test_avro_file(
                tmpdir, "file1.avro", schema, [{"id": i} for i in range(10)]
            )
            output_file = os.path.join(tmpdir, "output.avro")

            tool = ConcatTool()
            urls = [parse_url(file1)]
            output_url = parse_url(output_file)

            tool.concat(urls, output_url, "null")

            with avro_reader(output_url.with_mode("rb")) as reader:
                records = list(reader)
                assert len(records) == 10

    def test_concat_empty_file(self):
        """Graceful handling of empty file."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            # Create one empty and one non-empty file
            file1 = create_test_avro_file(tmpdir, "empty.avro", schema, [])
            file2 = create_test_avro_file(
                tmpdir, "file2.avro", schema, [{"id": 1}, {"id": 2}]
            )

            output_file = os.path.join(tmpdir, "output.avro")

            tool = ConcatTool()
            urls = [parse_url(file1), parse_url(file2)]
            output_url = parse_url(output_file)

            tool.concat(urls, output_url, "null")

            with avro_reader(output_url.with_mode("rb")) as reader:
                records = list(reader)
                assert len(records) == 2


class TestCatTool:
    """Tests for cat tool (Priority 2 - 37% coverage)."""

    def test_cat_offset(self):
        """Skip first N records."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [{"id": i} for i in range(10)]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)

            output_file = os.path.join(tmpdir, "output.avro")

            tool = CatTool()
            tool.sample(
                [parse_url(file_path)],
                parse_url(output_file),
                schema,
                codec="null",
                offset=3,
            )

            with avro_reader(parse_url(output_file).with_mode("rb")) as reader:
                output_records = list(reader)

            assert len(output_records) == 7
            assert output_records[0]["id"] == 3

    def test_cat_limit(self):
        """Stop after N records."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [{"id": i} for i in range(10)]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)

            output_file = os.path.join(tmpdir, "output.avro")

            tool = CatTool()
            tool.sample(
                [parse_url(file_path)],
                parse_url(output_file),
                schema,
                codec="null",
                limit=4,
            )

            with avro_reader(parse_url(output_file).with_mode("rb")) as reader:
                output_records = list(reader)

            assert len(output_records) == 4

    def test_cat_samplerate_zero(self):
        """No records returned when sample rate is 0."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [{"id": i} for i in range(10)]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)

            output_file = os.path.join(tmpdir, "output.avro")

            tool = CatTool()
            tool.sample(
                [parse_url(file_path)],
                parse_url(output_file),
                schema,
                codec="null",
                samplerate=0.0,
            )

            with avro_reader(parse_url(output_file).with_mode("rb")) as reader:
                output_records = list(reader)

            assert len(output_records) == 0

    def test_cat_samplerate_full(self):
        """All records returned when sample rate is 1.0."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [{"id": i} for i in range(10)]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)

            output_file = os.path.join(tmpdir, "output.avro")

            tool = CatTool()
            tool.sample(
                [parse_url(file_path)],
                parse_url(output_file),
                schema,
                codec="null",
                samplerate=1.0,
            )

            with avro_reader(parse_url(output_file).with_mode("rb")) as reader:
                output_records = list(reader)

            assert len(output_records) == 10

    def test_cat_offset_beyond_length(self):
        """Edge case: offset beyond file length."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [{"id": i} for i in range(5)]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)

            output_file = os.path.join(tmpdir, "output.avro")

            tool = CatTool()
            tool.sample(
                [parse_url(file_path)],
                parse_url(output_file),
                schema,
                codec="null",
                offset=100,
            )

            with avro_reader(parse_url(output_file).with_mode("rb")) as reader:
                output_records = list(reader)

            assert len(output_records) == 0

    def test_cat_deflate_codec(self):
        """Different codec handling (deflate)."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [{"id": i} for i in range(10)]
            file_path = create_test_avro_file(
                tmpdir, "test.avro", schema, records, codec="deflate"
            )

            output_file = os.path.join(tmpdir, "output.avro")

            tool = CatTool()
            tool.sample(
                [parse_url(file_path)],
                parse_url(output_file),
                schema,
                codec="deflate",
            )

            with avro_reader(parse_url(output_file).with_mode("rb")) as reader:
                output_records = list(reader)

            assert len(output_records) == 10


class TestGetSchemaTool:
    """Tests for getschema tool (Priority 2 - 53% coverage)."""

    def test_getschema_empty_file(self):
        """Error handling for empty file."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = create_test_avro_file(tmpdir, "empty.avro", schema, [])
            url = parse_url(file_path)

            from avrokit.io.schema import read_avro_schema_from_first_nonempty_file

            # Empty file should still have schema
            result = read_avro_schema_from_first_nonempty_file(url.expand())
            assert result is not None

    def test_getschema_complex_schema(self):
        """Complex schemas with nested records, unions, and enums."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [
                    {"name": "id", "type": "int"},
                    {
                        "name": "status",
                        "type": {
                            "type": "enum",
                            "name": "Status",
                            "symbols": ["A", "B", "C"],
                        },
                    },
                    {
                        "name": "data",
                        "type": {
                            "type": "record",
                            "name": "Data",
                            "fields": [{"name": "value", "type": "string"}],
                        },
                    },
                    {"name": "optional", "type": ["string", "null"]},
                    {"name": "tags", "type": {"type": "array", "items": "string"}},
                ],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            records = [
                {
                    "id": 1,
                    "status": "A",
                    "data": {"value": "test"},
                    "optional": "hello",
                    "tags": ["a", "b"],
                }
            ]
            file_path = create_test_avro_file(tmpdir, "test.avro", schema, records)
            url = parse_url(file_path)

            from avrokit.io.schema import read_avro_schema_from_first_nonempty_file

            extracted = read_avro_schema_from_first_nonempty_file([url])
            assert extracted is not None
            assert extracted.name == "Test"
            json_schema = extracted.to_json()
            assert "Data" in str(json_schema)
            assert "Status" in str(json_schema)


class TestPartitionedAvroWriter:
    """Tests for io/writer.py (Priority 3 - 90% coverage)."""

    def test_partitioned_writer_find_last_filename_none(self):
        """No existing files - returns None."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            url = parse_url(tmpdir, mode="wb")

            writer = PartitionedAvroWriter(url, schema)
            result = writer.find_last_filename()

            assert result is None

    def test_partitioned_writer_next_filename_invalid(self):
        """Malformed filename raises ValueError."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            url = parse_url(tmpdir, mode="wb")

            writer = PartitionedAvroWriter(url, schema)

            with pytest.raises(ValueError, match="Invalid filename format"):
                writer.next_filename("invalid_name.avro")

    def test_partitioned_writer_roll(self):
        """File rotation with roll()."""
        schema = avro_schema(
            {
                "type": "record",
                "name": "Test",
                "fields": [{"name": "id", "type": "int"}],
            }
        )

        with tempfile.TemporaryDirectory() as tmpdir:
            url = parse_url(tmpdir, mode="wb")

            writer = PartitionedAvroWriter(url, schema)

            with writer:
                writer.append({"id": 1})
                # Roll to new file
                writer.roll()
                writer.append({"id": 2})

            # Check that multiple files were created
            files = os.listdir(tmpdir)
            assert len(files) == 2


class TestBlockingQueueAvroReader:
    """Tests for asyncio/reader.py (Priority 3 - 90% coverage)."""

    def test_reader_worker_exception(self):
        """Exception path in reader thread."""

        def raise_error():
            raise ValueError("Test error")

        reader = BlockingQueueAvroReader(raise_error)
        reader.start()

        import time

        time.sleep(0.1)

        assert reader._reader_thread_done.is_set()


class TestDeferredAvroWriter:
    """Tests for asyncio/writer.py (Priority 3 - 90% coverage)."""

    def test_writer_worker_exception(self):
        """Exception path in writer thread."""

        class FailingWriter:
            def append(self, datum):
                raise ValueError("Write error")

        writer = DeferredAvroWriter(FailingWriter())
        writer.start()

        # Add a record that will fail
        writer.append({"id": 1})

        import time

        time.sleep(0.2)

        writer.stop()

        assert writer._writer_thread_done.is_set()
